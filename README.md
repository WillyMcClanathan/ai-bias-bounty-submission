# ğŸŒ¸ ai-bias-bounty-submission

**Submission for AI Bias Bounty Hackathon 2025 â€“ Loan Approval Bias Mitigation**  
**Track:** Fairness in Machine Learning | **Category:** Bias Mitigation Â· Explainability

---

## ğŸ“Œ Overview

This project tackles bias in loan approval models by identifying and mitigating unfair disparities across gender groups. It leverages advanced fairness-aware learning and explainability techniques to produce a more equitable model **without sacrificing performance**.

---

## ğŸ“ Files Included

- ğŸ““ `Bias Notebook (Final).ipynb` â€“ Full end-to-end notebook (preprocessing, bias detection, mitigation, SHAP)
- ğŸ“„ `submission (Final).csv` â€“ Final predictions after mitigation
- ğŸ“Š `fairness_metrics_best_plot.png` â€“ Visualization of fairness metrics by gender
- ğŸ“ˆ `Model_Explainability_SHAP.png` â€“ SHAP summary plot for model interpretability
- ğŸ§¾ `README.md` â€“ Project documentation

---

## ğŸ§  Technical Approach

- **Data:** Simulated structured loan approval dataset with protected attribute: `Gender`
- **Model:** `LogisticRegression` (baseline) with Fairlearnâ€™s `ExponentiatedGradient` for bias mitigation
- **Bias Metrics:**
  - âš–ï¸ Demographic Parity Difference
  - ğŸ“‰ Equalized Odds Difference
- **Fairness Toolkits:**
  - [Fairlearn](https://fairlearn.org/)
  - [SHAP](https://github.com/shap/shap)

---

## ğŸš¨ Bias Detection (Pre-Mitigation)

Before mitigation, significant disparities existed across gender groups in **selection rate** and **equalized odds**.

| Gender | Accuracy | Selection Rate |
|--------|----------|----------------|
| Male   | 0.628    | 0.657          |
| Female | 0.619    | 0.631          |
| Other  | 0.640    | 0.658          |

- ğŸŸ¥ **Demographic Parity Difference:** `0.0257`  
- ğŸŸ¥ **Equalized Odds Difference:** `0.0284`

---

## âœ… Post-Mitigation Results

After applying the `Exponentiated Gradient` algorithm, disparities were significantly reduced:

| Gender | Accuracy | Selection Rate |
|--------|----------|----------------|
| Male   | 0.630    | 0.660          |
| Female | 0.627    | 0.661          |
| Other  | 0.636    | 0.657          |

- ğŸŸ© **Demographic Parity Difference:** `0.0037`  
- ğŸŸ© **Equalized Odds Difference:** `0.0089`

> These results show that **fairness improved significantly** with minimal compromise in overall model performance.

---

## ğŸ“Š Fairness Visualization

Visual comparison of post-mitigation **accuracy and selection rates by gender**:

![Fairness Metrics](visualizations/fairness_metrics_best_plot.png)

---

## ğŸ“˜ Explainability with SHAP

Understanding why a model makes certain predictions is crucial in high-stakes scenarios like loan approvals. To that end, **SHAP** was used to interpret the modelâ€™s output.

### ğŸ” Key Benefits:
- Identifies the most influential features behind loan approval decisions
- Helps detect hidden bias in sensitive features (e.g., Gender, Age)
- Increases trust and accountability in model deployment

**SHAP Summary Plot:**

![Model Explainability - SHAP](visualizations/Model_Explainability_SHAP.png)

---

## ğŸ† Key Takeaways

- **Bias mitigation with Fairlearn** was effective in improving group fairness metrics.
- **SHAP interpretability** offered granular insights into model decision-making.
- The model maintains high accuracy while improving equity across gender groups.
- This pipeline is extendable to other protected features (e.g., race, age).

---

## ğŸ› ï¸ Environment

- Python 3.10
- scikit-learn 1.4+
- fairlearn 0.9.0
- shap 0.43+
- pandas, matplotlib, seaborn

---

## ğŸ“„ AI Risk Report

You can view the full fairness audit and bias analysis in the AI Risk Report:

[ğŸ“¥ Download Report (PDF)](./Report/ai_risk_report.pdf)

---

## ğŸ“„ License

MIT License

---

## ğŸ‘¤ Author

**Willy McClanathan**  
Mountlake Terrace, WA | Class of 2027  
TSA | Computer Science | AI Research â€¢ [GitHub Profile](https://github.com/WillyMcClanathan)
